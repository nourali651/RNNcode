{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5d560a-5dff-45ba-863d-57b805571049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1.384336632336534\n",
      "Epoch 50 loss: 0.08870752083211832\n",
      "Epoch 100 loss: 0.028260762884735644\n",
      "Epoch 150 loss: 0.015579188559756974\n",
      "Epoch 200 loss: 0.010468340725008945\n",
      "Epoch 250 loss: 0.007781969340623471\n",
      "Epoch 300 loss: 0.00614890258115382\n",
      "Epoch 350 loss: 0.0050603499143474584\n",
      "Epoch 400 loss: 0.004287076265189094\n",
      "Epoch 450 loss: 0.0037115645214061007\n",
      "\n",
      "After training:\n",
      "Predicted 4th word: zebra\n",
      "Actual 4th word: zebra\n"
     ]
    }
   ],
   "source": [
    "# === 1. Define helper math functions ===\n",
    "def exp(x):\n",
    "    n = 20  # number of terms in Taylor series\n",
    "    result = 1.0\n",
    "    term = 1.0\n",
    "    for i in range(1, n):\n",
    "        term *= x / i\n",
    "        result += term\n",
    "    return result\n",
    "\n",
    "def tanh(x):\n",
    "    e_pos = exp(x)\n",
    "    e_neg = exp(-x)\n",
    "    return (e_pos - e_neg) / (e_pos + e_neg)\n",
    "\n",
    "def log(x):\n",
    "    n = 100\n",
    "    result = 0\n",
    "    y = (x - 1) / (x + 1)\n",
    "    y2 = y * y\n",
    "    for i in range(1, n, 2):\n",
    "        result += (1/i) * (y ** i)\n",
    "    return 2 * result\n",
    "\n",
    "# === 2. Data Preparation ===\n",
    "text = [\"dogs\", \"giraffe\", \"cats\", \"zebra\"]\n",
    "vocab = list(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
    "ix_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "def one_hot(ix, size):\n",
    "    vec = [0 for _ in range(size)]\n",
    "    vec[ix] = 1\n",
    "    return vec\n",
    "\n",
    "# === 3. Initialize Weights manually (fixed small values) ===\n",
    "input_size = vocab_size\n",
    "hidden_size = 5\n",
    "output_size = vocab_size\n",
    "\n",
    "def rand():\n",
    "    rand.seed = (rand.seed * 1103515245 + 12345) % (2**31)\n",
    "    return ((rand.seed >> 16) & 0x7FFF) / 32767\n",
    "\n",
    "rand.seed = 42\n",
    "\n",
    "Wxh = [[(rand() - 0.5) * 0.2 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "Whh = [[(rand() - 0.5) * 0.2 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "Why = [[(rand() - 0.5) * 0.2 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "\n",
    "bh = [0 for _ in range(hidden_size)]\n",
    "by = [0 for _ in range(output_size)]\n",
    "\n",
    "# === 4. Helper Functions ===\n",
    "def matmul(vec, mat):\n",
    "    return [sum(vec[j] * mat[j][i] for j in range(len(vec))) for i in range(len(mat[0]))]\n",
    "\n",
    "def add(vec1, vec2):\n",
    "    return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "def softmax(vec):\n",
    "    exps = [exp(v) for v in vec]\n",
    "    sum_exps = sum(exps)\n",
    "    return [e / sum_exps for e in exps]\n",
    "\n",
    "def cross_entropy(pred, target_ix):\n",
    "    return -log(pred[target_ix] + 1e-9)\n",
    "\n",
    "def argmax(vec):\n",
    "    return vec.index(max(vec))\n",
    "\n",
    "def tanh_vec(vec):\n",
    "    return [tanh(v) for v in vec]\n",
    "\n",
    "def matTmul(vec, mat):\n",
    "    return [sum(vec[j] * mat[i][j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "# === 5. Training Loop ===\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(500):\n",
    "    h = [0 for _ in range(hidden_size)]\n",
    "    inputs = text[:3]\n",
    "    target = word_to_ix[text[3]]\n",
    "    xs, hs = [], [h]\n",
    "\n",
    "    for word in inputs:\n",
    "        x = one_hot(word_to_ix[word], vocab_size)\n",
    "        xs.append(x)\n",
    "        h = tanh_vec(add(add(matmul(x, Wxh), matmul(h, Whh)), bh))\n",
    "        hs.append(h)\n",
    "\n",
    "    y = add(matmul(h, Why), by)\n",
    "    p = softmax(y)\n",
    "\n",
    "    loss = cross_entropy(p, target)\n",
    "\n",
    "    dWhy = [[0 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "    dWhh = [[0 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "    dWxh = [[0 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "    dbh = [0 for _ in range(hidden_size)]\n",
    "    dby = [0 for _ in range(output_size)]\n",
    "\n",
    "    dh_next = [0 for _ in range(hidden_size)]\n",
    "\n",
    "    dy = p[:]\n",
    "    dy[target] -= 1\n",
    "\n",
    "    for i in range(hidden_size):\n",
    "        for j in range(output_size):\n",
    "            dWhy[i][j] += hs[-1][i] * dy[j]\n",
    "    for j in range(output_size):\n",
    "        dby[j] += dy[j]\n",
    "\n",
    "    dh = matTmul(dy, Why)\n",
    "    dh = add(dh, dh_next)\n",
    "\n",
    "    for t in range(2, -1, -1):\n",
    "        h_raw = hs[t+1]\n",
    "        dh_raw = [(1 - h_raw[i] ** 2) * dh[i] for i in range(hidden_size)]\n",
    "\n",
    "        for i in range(hidden_size):\n",
    "            for j in range(hidden_size):\n",
    "                dWhh[j][i] += hs[t][j] * dh_raw[i]\n",
    "            for j in range(input_size):\n",
    "                dWxh[j][i] += xs[t][j] * dh_raw[i]\n",
    "            dbh[i] += dh_raw[i]\n",
    "\n",
    "        dh = matTmul(dh_raw, Whh)\n",
    "\n",
    "    for i in range(input_size):\n",
    "        for j in range(hidden_size):\n",
    "            Wxh[i][j] -= learning_rate * dWxh[i][j]\n",
    "    for i in range(hidden_size):\n",
    "        for j in range(hidden_size):\n",
    "            Whh[i][j] -= learning_rate * dWhh[i][j]\n",
    "    for i in range(hidden_size):\n",
    "        for j in range(output_size):\n",
    "            Why[i][j] -= learning_rate * dWhy[i][j]\n",
    "    for i in range(hidden_size):\n",
    "        bh[i] -= learning_rate * dbh[i]\n",
    "    for i in range(output_size):\n",
    "        by[i] -= learning_rate * dby[i]\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch\", epoch, \"loss:\", loss)\n",
    "\n",
    "# === 6. Inference ===\n",
    "print(\"\\nAfter training:\")\n",
    "\n",
    "h = [0 for _ in range(hidden_size)]\n",
    "for word in text[:3]:\n",
    "    x = one_hot(word_to_ix[word], vocab_size)\n",
    "    h = tanh_vec(add(add(matmul(x, Wxh), matmul(h, Whh)), bh))\n",
    "\n",
    "y = add(matmul(h, Why), by)\n",
    "p = softmax(y)\n",
    "predicted_ix = argmax(p)\n",
    "\n",
    "print(\"Predicted 4th word:\", ix_to_word[predicted_ix])\n",
    "print(\"Actual 4th word:\", text[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae5f2c-44cd-41b8-a77e-942a57499e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
